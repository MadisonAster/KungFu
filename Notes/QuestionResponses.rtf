{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\colortbl ;\red0\green0\blue255;}
{\*\generator Riched20 10.0.19041}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\f0\fs22\lang9 In your own words and as much technical detail as you can, please answer the following questions:\par
\b\par
1. Explain the project(s) you\rquote ve worked on in a technical leadership capacity that ismost applicable the role and/or skills required?\par
\b0 At Cognition LA I led a small team to pull together a VR travel application demo for a tradeshow known as VRLA. With 5 artists and one other engineer we were able to able to achieve the following in 3 months:\par
 - Support for Oculus and Vive. Simultaneous support for both of the major VR devices was tougher in the early days. We built a series of abstract base classes in Unreal Engine to handle the different ways both devices calculate position and rotation, as well as a universal menu system with various experimental movement and camera options. I worked with, and helped our talented team of artists construct a number of maps for the platform such as a reconstruction of the Apollo moon landing site, and a famous Icelandic cave. In addition to leveraging my 3D talents, I was unfortunately the only member of the team that understood how to use source control at the time. So in addition to coordinating our efforts, programming, and working with the artists, I was also responsible for checking in assets and resolving file conflicts.\par
- We built photogrammtery reconstructions of destroyed cultural heritage sites in Syria. In the months we spent developing this, ISIL was releasing videos where they were destroying a roman ampitheater with sledge hammers in Palmyra Syria. We were able to successfully reconstruct the original site using only tourist photographs in Agisoft Photoscan. The initial result was rough, but because the ancient romans had such a love for mathematical proportions, from that initial solve we were able to precisely reconstruct the original measurements and create  reference geometry to feed back into the photogrammetry pipeline. This is by far one of the most amazing accomplishments of my career.\par
- We also built and operated a booth for a tradeshow known as VRLA. As a startup, we couldn't afford to hire a company to build a booth for us. And we also needed to make sure we had all of the cables necessary, and hide a server rack in the milddle of the booth. So I put my film school pre production and grip experience to work building a project plan. I leveraged every resource I could get to make sure everything arrived on time and was built correctly for the opening day of the conference. I even put the office accountant to work.\b\par
\par
\par
2. How would you architect a pipeline for that allows artists to author cg assets in a3d package like maya/3dsmax/houdini/blender and export them to multiple mobilegame engines: scenekit, sceneform and three.js. Explain the file formats, mobileconsiderations, tools and workflows you would put in place to make thispossible?\par
\b0 The biggest challenge facing any real-time rendering pipeline is asset preparation, and sanitization. VFX artists, in particular, often come from a world where triangle count doesn't matter. UVs, and LODs need to be constructed in advance, especially as you approach lower and lower end devices. To take advantage of the fantastic skills that VFX artists posess, and bring them into a realtime game engine environment at Saatchi & Saatchi, I built a system I called "The Factory Line". \par
The goal of this system was an end-to-end fully automated asset preparation pipeline. I leveraged the tessellation algorithms available in a number of commercial 3D applications, ZBrush, Unreal, and Alias. I then built a series of microservices inside of a Deadline render farm to pass data from stage to stage as it was being processed. We pushed through 9 different versions for each piece of geometry, 3 different settings across 3 different packages, and then fed them into another microservice we called The Trainer. This was a simple Qt application, where all 9 versions of the geometry would be displayed in 9 different opengl viewports, and a human "trainer" could then select the best version. The service was then responsible marking the task as complete, and then automatically opening up the next 9 files that needed grading. The plan was to replace the human input with an AI once we had enough training data, however the decision making process became so fast that it didn't make economic sense to pursue an AI even at the scale we were operating at with Toyota.\par
Since the target for your company is AR, and shaders will need to be highly optimized, we would also need to add a microservice to act as a custom shader language compiler to mimic the results of any shader program that an artist wanted to use Arnold, Vray, et al. With the right AI engineer, we could build a system that trained generated results against the commercially rendered output, and tries out different networks of texture inputs and known BDRF's to strike the right balance between matching the artist's intent, and optimizing for performance.\par
Resulting FBX files and compiled shaders should then be published to a distributed database system to deliver as content to the end user applications that we build. When making devops choices such as this I think the most important thing is to choose the solution that's already being broadly used within your organization. Based on my brief research into your company, that seems to be Microsoft Azure.\b\par
\par
\par
3. What, in your opinion, are the strengths and weaknesses of today\rquote s ARtechnology on iOS, Android and Web. Be as detailed as you can?\par
\b0 The inevitable direction AR will push us towards is containerization of shader rendering, and the streaming of fragment shader results down to your mobile device for final assembly and pixel shading. I believe that Docker and Kubernetes will be a major part of the answer to the containerization and cluster computing that will be necessary to realize the full potential of AR experiences. If this sounds far flung into the future to you, you should check out some of the existing work in this field at {{\field{\*\fldinst{HYPERLINK https://unrealcontainers.com/ }}{\fldrslt{https://unrealcontainers.com/\ul0\cf0}}}}\f0\fs22\par
Once AR glasses become widely available and convenient, the key will be pairing each user with a cluster, and adding/removing 3D rendering containers to that cluster as the user walks down the street and enters/exits the range of various virtual points of interest. The glasses will send orientation and position data to the phone, and the phone will send that to the cluster. Each container added to the cluster will render its fragment shader using that camera data, and stream its results back to the phone. The phone will combine the results of the streaming fragments in a pixel shader, and use its low latency proximity to the glasses to correct the orientations of the fragments before compositing them. Then the final results of the pixel shader will be streamed to the glasses.\par
So what's the weakness? We don't have glasses yet. Phones by themselves are fine for testing, but they're awkward and dangerous to hold in front of your face as you walk down the street. 5G will definitely play a major part in being able to stream 3D fragments in this fashion.\b\par
4. When attempting to reach photorealism on mobile devices, what are some of thetradeoffs you have to make and what are some of the optimizations you\rquote veemployed in the past to make this possible. Please be as detailed as you caninclude links to any visuals if you have them?\b0\par
The biggest obstacle in any low poly scenario is geometric edges. Using fragment shaders to add pixel space offsets, and pixel shaders to blur the edges and add light wrapping effects is key. Real time compositing is cheaper than adding triangles when used effectively, and the draw call has to be made anyway so why not get the most out of it?\par
It's also really important to start with great textures. Incorporating photogrammetry into your asset pipeline is a must. Textures never look more real than when they come from a high quality photogrammetry solve.\b\par
\b0 And finally, if you want true photorealism, the first and best trick in the bag is to desaturate the image. Instagram has tricked a lot of people into thinking the world is far more vivid and saturated than it is, and artists have a tendency to want to make everything pop. Chasing photorealism is about hiding your work, not drawing attention to it.\b\par
\b0\par
}
 